# convert class to factor
train$cluter = as.factor(train$cluster)
install.packages("randomForest")
# load library
library(randomForest)
View(train)
# select two first PC
pc = pca$x[,1:2]
pc.df = as.data.frame(pc)
pc.df$cluster = kmeans.3$cluster
pc.df$tile_id = metric.df$tile_id
# Set random seed to make results reproducible:
set.seed(200)
# split data into two 80% for training and 20% for testing
sample <- sample(2, nrow(pc.df), replace=T, prob=c(0.8,0.2))
train<- pc.df[sample==1,]
test<- pc.df[sample==2,]
nrow(train)
nrow(test)
# convert class to factor
train$cluster = as.factor(train$cluster)
View(test)
View(train)
rownames(pc.df) = metric.df$tile_id
View(pc.df)
# select two first PC
pc = pca$x[,1:2]
pc.df = as.data.frame(pc)
pc.df$cluster = kmeans.3$cluster
rownames(pc.df) = metric.df$tile_id
# Set random seed to make results reproducible:
set.seed(200)
# split data into two 80% for training and 20% for testing
sample <- sample(2, nrow(pc.df), replace=T, prob=c(0.8,0.2))
train<- pc.df[sample==1,]
test<- pc.df[sample==2,]
nrow(train)
nrow(test)
View(train)
# convert class to factor
train$cluster = as.factor(train$cluster)
View(train)
# select two first PC
pc = pca$x[,1:2]
pc.df = as.data.frame(pc)
pc.df$cluster = kmeans.3$cluster
pc.df$tile_id = metric.df$tile_id
# Set random seed to make results reproducible:
set.seed(200)
# split data into two 80% for training and 20% for testing
sample <- sample(2, nrow(pc.df), replace=T, prob=c(0.8,0.2))
train<- pc.df[sample==1,]
test<- pc.df[sample==2,]
nrow(train)
nrow(test)
# convert class to factor
train$cluster = as.factor(train$cluster)
View(train)
View(train)
model.rf = randomForest(cluster ~ PC1 + PC2,
data = train,
keep.forest = T,
importance = T,
proximity = T)
model.rf
# visualize the out-of-bag error
# create a dataframe for visulizing the error rate
oob.df = data.frame(
Tree=rep(1:nrow(model.rf$err.rate), times=3),
Type = rep(c('OOB','oak','pine'), each=nrow(model.rf$err.rate)),
Error=c(model.rf$err.rate[,'OOB'],
model.rf$err.rate[,'oak'],model.rf$err.rate[,'pine'])
)
model.rf$err.rate
# visualize the out-of-bag error
# create a dataframe for visulizing the error rate
oob.df = data.frame(
Tree=rep(1:nrow(model.rf$err.rate), times=3),
Type = rep(c('OOB','1','2','3'), each=nrow(model.rf$err.rate)),
Error=c(model.rf$err.rate[,'OOB'], model.rf$err.rate[,'1'],
model.rf$err.rate[,'2'],model.rf$err.rate[,'3'])
)
# visualize the out-of-bag error
# create a dataframe for visulizing the error rate
oob.df = data.frame(
Tree=rep(1:nrow(model.rf$err.rate), times=3),
Type = rep(c('OOB','1','2','3'), each=nrow(model.rf$err.rate)),
Error=c(model.rf$err.rate[,'OOB'], model.rf$err.rate[,'1'],
model.rf$err.rate[,'2'],model.rf$err.rate[,'3'])
)
model.rf$err.rate
nrow(model.rf$err.rate)
is.na(model.rf$err.rate)
# visualize the out-of-bag error
# create a dataframe for visulizing the error rate
oob.df = data.frame(
Tree=rep(1:nrow(model.rf$err.rate), times=3),
Type = rep(c('OOB','1','2','3'), each=nrow(model.rf$err.rate)),
Error=c(model.rf$err.rate[,'OOB'], model.rf$err.rate[,'1'],
model.rf$err.rate[,'2'],model.rf$err.rate[,'3'])
)
oob.df = data.frame(Tree=rep(1:nrow(model.rf$err.rate), times=3))
View(oob.df)
rep(c('OOB','1','2','3'), each=nrow(model.rf$err.rate))
View(oob.df)
# visualize the out-of-bag error
# create a dataframe for visulizing the error rate
oob.df = data.frame(
Tree=rep(1:nrow(model.rf$err.rate), times=4),
Type = rep(c('OOB','1','2','3'), each=nrow(model.rf$err.rate)),
Error=c(model.rf$err.rate[,'OOB'], model.rf$err.rate[,'1'],
model.rf$err.rate[,'2'],model.rf$err.rate[,'3'])
)
View(oob.df)
ggplot(data=oob.df,aes(x=Tree, y=Error))+
geom_line(aes(color=Type))
library(ggplot2)
ggplot(data=oob.df,aes(x=Tree, y=Error))+
geom_line(aes(color=Type))
# create a dataframe for visulizing the error rate
oob.df = data.frame(
Tree=rep(1:nrow(model.rf$err.rate), times=4),
Type = rep(c('OOB','Group 1',' Group 2','Group 3'), each=nrow(model.rf$err.rate)),
Error=c(model.rf$err.rate[,'OOB'], model.rf$err.rate[,'1'],
model.rf$err.rate[,'2'],model.rf$err.rate[,'3'])
)
ggplot(data=oob.df,aes(x=Tree, y=Error))+
geom_line(aes(color=Type))
# visualize the out-of-bag error
# create a dataframe for visulizing the error rate
oob.df = data.frame(
Tree=rep(1:nrow(model.rf$err.rate), times=4),
Type = rep(c('OOB','Group 1','Group 2','Group 3'), each=nrow(model.rf$err.rate)),
Error=c(model.rf$err.rate[,'OOB'], model.rf$err.rate[,'1'],
model.rf$err.rate[,'2'],model.rf$err.rate[,'3'])
)
ggplot(data=oob.df,aes(x=Tree, y=Error))+
geom_line(aes(color=Type))
ggplot(data=oob.df,aes(x=Tree, y=Error))+
geom_line(aes(color=Type))+theme_bw()
ggplot(data=oob.df,aes(x=Tree, y=Error))+
geom_line(aes(color=Type))+theme_bw()+ labs(x='Number of trees')
ggplot(data=oob.df,aes(x=Tree, y=Error))+
geom_line(aes(color=Type))+theme_bw()+ labs(x='Number of trees')+
ylim(0,0.75)
png(filename = paste0(folder_result,"13_randomForest/01_oob.png"),
width = 600, height = 600, res = 120)
ggplot(data=oob.df,aes(x=Tree, y=Error))+
geom_line(aes(color=Type))+theme_bw()+ labs(x='Number of trees')+
ylim(0,0.75)
dev.off()
png(filename = paste0(folder_result,"13_randomForest/01_oob.png"),
width = 600, height = 500, res = 120)
ggplot(data=oob.df,aes(x=Tree, y=Error))+
geom_line(aes(color=Type))+theme_bw()+ labs(x='Number of trees')+
ylim(0,0.75)
dev.off()
# Now check to see if the random forest better with different parameters
# by comparing OOB value
# try different ntree of 200,400,600,800, and 1000
# try different mtry of 1, 2, 3, and 4
oob=c()
ntree = c(200,400,600,800,1000)
for (tree in ntree) {
for (nvariable in 1:2) {
model.rf = randomForest(cluster ~ PC1 + PC2,
data = train,
keep.forest = T,
importance = TRUE,
mtry = nvariable,
ntree = tree)
print(paste0("ntree = ",tree))
print(paste0('mtry = ',nvariable))
oob[length(oob)+1] = mean(model.rf$err.rate)
oob[length(oob)]
}
}
# adding colnames and rownnames for better visualization
oob = matrix(oob, ncol=2, byrow = T)
colnames(oob) = c('mtry = 1','mtry = 2')
rownames(oob) = c('ntree = 200','ntree = 400','ntree = 600','ntree = 800','ntree = 1000')
oob
# Now check to see if the random forest better with different parameters
# by comparing OOB value
# try different ntree of 200,400,600,800, and 1000
# try different mtry of 1, 2, 3, and 4
oob=c()
ntree = c(100,200,300,400,500)
for (tree in ntree) {
for (nvariable in 1:2) {
model.rf = randomForest(cluster ~ PC1 + PC2,
data = train,
keep.forest = T,
importance = TRUE,
mtry = nvariable,
ntree = tree)
print(paste0("ntree = ",tree))
print(paste0('mtry = ',nvariable))
oob[length(oob)+1] = mean(model.rf$err.rate)
oob[length(oob)]
}
}
# adding colnames and rownnames for better visualization
oob = matrix(oob, ncol=2, byrow = T)
colnames(oob) = c('mtry = 1','mtry = 2')
rownames(oob) = c('ntree = 200','ntree = 400','ntree = 600','ntree = 800','ntree = 1000')
oob
# adding colnames and rownnames for better visualization
oob = matrix(oob, ncol=2, byrow = T)
colnames(oob) = c('mtry = 1','mtry = 2')
rownames(oob) = c('ntree = 100','ntree = 200','ntree = 300','ntree = 400','ntree = 5000')
oob
mean(model.rf$err.rate)
model.rf = randomForest(cluster ~ PC1 + PC2,
data = train,
keep.forest = T,
importance = T,
proximity = T)
(model.rf$err.rate)
mean(model.rf$err.rate)
model.rf$err.rate
model.rf$err.rate[500,1]
for (tree in ntree) {
for (nvariable in 1:2) {
model.rf = randomForest(cluster ~ PC1 + PC2,
data = train,
keep.forest = T,
importance = TRUE,
mtry = nvariable,
ntree = tree)
print(paste0("ntree = ",tree))
print(paste0('mtry = ',nvariable))
oob[length(oob)+1] = mean(model.rf$err.rate)
oob[length(oob)]
}
}
oob[length(oob)]
oob
# Now check to see if the random forest better with different parameters
# by comparing OOB value
# try different ntree of 200,400,600,800, and 1000
# try different mtry of 1, 2, 3, and 4
oob=c()
ntree = c(100,200,300,400,500)
for (tree in ntree) {
for (nvariable in 1:2) {
model.rf = randomForest(cluster ~ PC1 + PC2,
data = train,
keep.forest = T,
importance = TRUE,
mtry = nvariable,
ntree = tree)
print(paste0("ntree = ",tree))
print(paste0('mtry = ',nvariable))
oob[length(oob)+1] = model.rf$err.rate[nrow(model.rf$err.rate),1]
oob[length(oob)]
}
}
# adding colnames and rownnames for better visualization
oob = matrix(oob, ncol=2, byrow = T)
oob
colnames(oob) = c('mtry = 1','mtry = 2')
rownames(oob) = c('ntree = 100','ntree = 200','ntree = 300','ntree = 400','ntree = 5000')
oob
model.rf = randomForest(cluster ~ PC1 + PC2,
data = train,
keep.forest = T,
importance = T,
proximity = T)
# Now check to see if the random forest better with different parameters
# by comparing OOB value
# try different ntree of 200,400,600,800, and 1000
# try different mtry of 1, 2, 3, and 4
oob=c()
ntree = c(100,200,300,400,500)
for (tree in ntree) {
for (nvariable in 1:2) {
model.rf = randomForest(cluster ~ PC1 + PC2,
data = train,
keep.forest = T,
importance = TRUE,
mtry = nvariable,
ntree = tree)
print(paste0("ntree = ",tree))
print(paste0('mtry = ',nvariable))
oob[length(oob)+1] = model.rf$err.rate[nrow(model.rf$err.rate),1]
oob[length(oob)]
}
}
# adding colnames and rownnames for better visualization
oob = matrix(oob, ncol=2, byrow = T)
colnames(oob) = c('mtry = 1','mtry = 2')
rownames(oob) = c('ntree = 100','ntree = 200','ntree = 300','ntree = 400','ntree = 5000')
oob
# final random forest with chosen parameter of mtry = 3 and ntree = 600
model.rf = randomForest(Species ~ DBH + H + V + Age,
data = train,
keep.forest = T,
importance = TRUE,
mtry = 1,
ntree = 300)
# final random forest with chosen parameter of mtry = 3 and ntree = 600
model.rf = randomForest(cluster ~ PC1 + PC2,
data = train,
keep.forest = T,
importance = TRUE,
mtry = 1,
ntree = 300)
model.rf
install.packages("caret")
library(caret)
# tree nodes distributions
hist(treesize(model.rf))
prp(model.rf)
plot(model.rf)
# plot importance of each predictor variables
varImpPlot(model.rf)
png(filename = paste0(folder_result,"13_randomForest/02_vaiableImportant.png"),
width = 600, height = 500, res = 120)
varImpPlot(model.rf)
dev.off()
png(filename = paste0(folder_result,"13_randomForest/02_vaiableImportant.png"),
width = 700, height = 500, res = 120)
varImpPlot(model.rf)
dev.off()
MDSplot(model.rf, train$cluster)
model.rf = randomForest(cluster ~ PC1 + PC2,
data = train,
keep.forest = T,
importance = TRUE,
proximity =T,
mtry = nvariable,
ntree = tree)
MDSplot(model.rf, train$cluster)
# classify the test dataset
prediction<-predict(model.rf, test ,type = 'class')
# accuracy assessment
confusionMatrix(prediction, test$cluster)
prediction
test
prediction
# accuracy assessment
confusionMatrix(prediction, test$cluster)
# accuracy assessment
confusionMatrix(prediction, test$cluster)
prediction
summary(prediction)
test
pc.df2 = pc.df[pc.df$tile_id==c('634_5608','637_5609')]
pc.df2 = pc.df[pc.df$tile_id==c('634_5608','637_5609'),]
View(pc.df2)
pc.df2 = pc.df[pc.df$tile_id=c('634_5608','637_5609'),]
View(pc.df)
summary(pc.df)
pc.df$tile_id==c('634_5608','637_5609')
pc.df2 = pc.df[pc.df$tile_id=="634_5608",]
pc.df2 = pc.df[pc.df$tile_id=="634_5608"/"637_5609",]
pc.df2 = pc.df[pc.df$tile_id=="634_5608" or "637_5609",]
pc.df2 = pc.df[pc.df$tile_id=="634_5608"/"637_5609",]
pc.df$tile_id
pc.df2 = pc.df[pc.df$tile_id["634_5608","637_5609"],]
pc.df$tile_id["634_5608","637_5609"]
pc.df$tile_id
pc.df$tile_id[c("634_5608","637_5609")]
pc.df2 = pc.df[pc.df$tile_id[c("634_5608","637_5609")],]
View(pc.df2)
pc.df$tile_id[c("634_5608","637_5609")]
pc.df$tile_id["634_5608","637_5609"]
pc.df$tile_id[["634_5608","637_5609"]]
pc.df$tile_id[["634_5608","637_5609"]]
pc.df$tile_id["634_5608"]
pc.df$tile_id
pc.df$tile_id["634_5608",]
pc.df$tile_id[1]
pc.df$tile_id["614_5608"]
pc.df2 = pc.df[pc.df$tile_id=='634_5608' and pc.df$tile_id== '637_5609',]
pc.df2 = pc.df[pc.df$tile_id=='634_5608' & pc.df$tile_id== '637_5609',]
View(pc.df2)
pc.df2 = pc.df[pc.df$tile_id=='634_5608',]
View(pc.df2)
pc.df2 = pc.df[pc.df$tile_id=='634_5608'&'637_5609',]
pc.df2 = pc.df[pc.df$tile_id==c('634_5608','637_5609'),]
pc.df2 = pc.df[pc.df$tile_id=c('634_5608','637_5609'),]
pc.df2 = subset(pc.df, tile_id==c('634_5608','637_5609'))
View(pc.df2)
pc.df2 = subset(pc.df, tile_id=='634_5608' & '637_5609')
pc.df2 = subset(pc.df, pc.df$tile_id =='634_5608' & '637_5609')
pc.df$tile_id==c('634_5608','637_5609')
pc.df2 = pc.df[which(pc.df$tile_id==c('634_5608','637_5609')),]
pc.df2 = pc.df[pc.df$tile_id=='634_5608',]
View(pc.df2)
View(pc.df2)
test
pc.df2 = pc.df[pc.df$tile_id=='634_5608'|pc.df$tile_id=='637_5609',]
View(pc.df2)
pc.df2 = pc.df[pc.df$tile_id=='634_5608'|'637_5609',]
pc.df2 = pc.df[pc.df$tile_id==c('634_5608'|'637_5609'),]
pc.df2 = pc.df[pc.df$tile_id=='634_5608'|'637_5609',]
pc.df2 = pc.df[pc.df$tile_id=='634_5608'|pc.df$tile_id=='637_5609',]
View(pc.df2)
pc.df
pc.df2
pca = prcomp(pc.df2[1:2], center = TRUE, scale. = TRUE)
# PCA
# by default, prcomp() expects the samples to be rows
# and the attributes to be columns
pca = prcomp(metric.df[2:28], center = TRUE, scale. = TRUE)
pca2 = prcomp(pc.df2[1:2], center = TRUE, scale. = TRUE)
plot(pca2$x[,1], pca2$x[,2])
pca2$x[,1]
pca2$x
View(pc.df)
plot(pca2$x[,1], pca2$x[,2])
prediction = predict(model.rf, pc.df2, type='class')
# classify the test dataset
prediction<-predict(model.rf, test ,type = 'class')
prediction2 = predict(model.rf, pc.df2, type='class')
prediction2
View(pc.df2)
View(test)
View(metric.df)
pc.df2 = metric.df[metric.df$tile_id=='634_5608'|metric.df$tile_id=='637_5609',]
View(pc.df2)
pca2 = prcomp(pc.df2[2:28], center = TRUE, scale. = TRUE)
plot(pca2$x[,1], pca2$x[,2])
pc.df2
pca2$x
pc = pca2$x[,1:2]
pc.df = as.data.frame(pc)
View(test)
metric.df2 = metric.df[metric.df$tile_id=='634_5608'|metric.df$tile_id=='637_5609',]
pca2 = prcomp(metric.df2[2:28], center = TRUE, scale. = TRUE)
plot(pca2$x[,1], pca2$x[,2])
pc2 = pca2$x[,1:2]
pc.df2 = as.data.frame(pc)
prediction2 = predict(model.rf, pc.df2, type='class')
prediction2
pc = pca$x[,1:2]
pc.df = as.data.frame(pc)
pc.df$cluster = kmeans.3$cluster
pc.df$tile_id = metric.df$tile_id
# Set random seed to make results reproducible:
set.seed(200)
# split data into two 80% for training and 20% for testing
sample <- sample(2, nrow(pc.df), replace=T, prob=c(0.8,0.2))
train<- pc.df[sample==1,]
test<- pc.df[sample==2,]
nrow(train)
nrow(test)
# convert class to factor
train$cluster = as.factor(train$cluster)
# load library
model.rf = randomForest(cluster ~ PC1 + PC2,
data = train,
keep.forest = T,
importance = T,
proximity = T)
model.rf
# visualize the out-of-bag error
# create a dataframe for visulizing the error rate
oob.df = data.frame(
Tree=rep(1:nrow(model.rf$err.rate), times=4),
Type = rep(c('OOB','Group 1',' Group 2','Group 3'), each=nrow(model.rf$err.rate)),
Error=c(model.rf$err.rate[,'OOB'], model.rf$err.rate[,'1'],
model.rf$err.rate[,'2'],model.rf$err.rate[,'3'])
)
png(filename = paste0(folder_result,"13_randomForest/01_oob.png"),
width = 800, height = 500, res = 120)
ggplot(data=oob.df,aes(x=Tree, y=Error))+
geom_line(aes(color=Type))+theme_bw()+ labs(x='Number of trees')+
ylim(0,0.75)
dev.off()
setwd("E:/OneDrive_HNEE/01_study/03_Master_FIT/04_semester_4/Environmental_data_analysis/exam")
# set working directory
setwd("E:/OneDrive_HNEE/01_study/03_Master_FIT/04_semester_4/Environmental_data_analysis/exam")
getwd()
The study area covers an area of 20 km2 located in the Free State of Thuringia, central Germany (Fig. 1). The state covers 16,171 km2 with a population of about 2.1 million. Due to its extensive, dense forest, it has been referred to as "the green heart of Germany" since the 19th century (Wikipedia, 2022). Thuringia's original natural vegetation was a forest with beech as the main species. A blend of beech and spruce would be typical in the uplands. However, most of the forests are spruce and pine-planted, while most of the plains have been cleared and are being used for intensive agriculture. Since 1990, Thuringia's forests have been maintained to create tougher, more natural vegetation that is resistant to pests and disease (Wikipedia, 2022). The average daily high temperature of Thuringia, one of Germany's coldest regions, is only 12oC. The weather is generally consistent with that of Central Europe (Worlddata.info, 2022).
![](result/04_map)
![](/result/04_map)
![pic](/result/04_map)
![pic](result/04_map/studyArea_map.png)
getwd()
![](/result/04_map/studyArea_map.png)
This section describes the analysis procedures used in the study. The flow diagram outlines the methodology can be found in Figure 2. All calculations and analyses were conducted using the R environment with R studio 2022.07 and R version 4.2.1. Mapping was created by QGIS 3.24.
getwd()
setwd("E:/OneDrive_HNEE/01_study/03_Master_FIT/04_semester_4/Environmental_data_analysis/exam")
getwd()
# prepare workspace
folder_data = paste0(getwd(),"/00_data/Mehrfachdownload_rIYYX7_2VX73u/")
folder_result = paste0(getwd(),"/result/")
#### import file xyz
# get list of file
file.list = list.files(path = folder_data, pattern = ".xyz")
getwd()
#### import file xyz
# get list of file
file.list = list.files(path = folder_data, pattern = ".xyz")
file.list
folder_data
list.files(path = folder_data)
getwd()
# prepare workspace
folder_data = paste0(getwd(),"/00_data/Mehrfachdownload_rIYYX7_2VX73u/")
folder_result = paste0(getwd(),"/result/")
#### import file xyz
# get list of file
file.list = list.files(path = folder_data, pattern = ".xyz")
file.list
folder_data
getwd()
# set working directory
setwd("E:/OneDrive_HNEE/01_study/03_Master_FIT/04_semester_4/Environmental_data_analysis/exam")
