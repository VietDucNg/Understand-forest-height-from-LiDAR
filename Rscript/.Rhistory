width = 600, height = 500, res = 120)
prp(decision_tree, space = 2, split.cex = 1.5, nn.border.col = 1)
dev.off()
test
test
test
test
test
train
train
train
View(metric.df)
View(metric.df)
metric.df[2:28]
View(metric.df)
View(metric.df)
pca2 = prcomp(metric.df[1:2,2:28])
pca2$x
View(metric.df)
View(pca2)
View(pc.df)
plot(pca2)
View(test)
metric.df["634_5608",]
metric.df[metric.df$tile_id='634_5608',]
metric.df[metric.df$tile_id=='634_5608',]
rm(pca2)
# select two first PC
pc = pca$x[,1:2]
pc.df = as.data.frame(pc)
pc.df$cluster = kmeans.3$cluster
pc.df$tile_id = metric.df$tile_id
# Set random seed to make results reproducible:
set.seed(200)
# split data into two 80% for training and 20% for testing
sample <- sample(2, nrow(pc.df), replace=T, prob=c(0.8,0.2))
train<- pc.df[sample==1,]
test<- pc.df[sample==2,]
nrow(train)
nrow(test)
# convert class to factor
train$cluter = as.factor(train$cluster)
install.packages("randomForest")
# load library
library(randomForest)
View(train)
# select two first PC
pc = pca$x[,1:2]
pc.df = as.data.frame(pc)
pc.df$cluster = kmeans.3$cluster
pc.df$tile_id = metric.df$tile_id
# Set random seed to make results reproducible:
set.seed(200)
# split data into two 80% for training and 20% for testing
sample <- sample(2, nrow(pc.df), replace=T, prob=c(0.8,0.2))
train<- pc.df[sample==1,]
test<- pc.df[sample==2,]
nrow(train)
nrow(test)
# convert class to factor
train$cluster = as.factor(train$cluster)
View(test)
View(train)
rownames(pc.df) = metric.df$tile_id
View(pc.df)
# select two first PC
pc = pca$x[,1:2]
pc.df = as.data.frame(pc)
pc.df$cluster = kmeans.3$cluster
rownames(pc.df) = metric.df$tile_id
# Set random seed to make results reproducible:
set.seed(200)
# split data into two 80% for training and 20% for testing
sample <- sample(2, nrow(pc.df), replace=T, prob=c(0.8,0.2))
train<- pc.df[sample==1,]
test<- pc.df[sample==2,]
nrow(train)
nrow(test)
View(train)
# convert class to factor
train$cluster = as.factor(train$cluster)
View(train)
# select two first PC
pc = pca$x[,1:2]
pc.df = as.data.frame(pc)
pc.df$cluster = kmeans.3$cluster
pc.df$tile_id = metric.df$tile_id
# Set random seed to make results reproducible:
set.seed(200)
# split data into two 80% for training and 20% for testing
sample <- sample(2, nrow(pc.df), replace=T, prob=c(0.8,0.2))
train<- pc.df[sample==1,]
test<- pc.df[sample==2,]
nrow(train)
nrow(test)
# convert class to factor
train$cluster = as.factor(train$cluster)
View(train)
View(train)
model.rf = randomForest(cluster ~ PC1 + PC2,
data = train,
keep.forest = T,
importance = T,
proximity = T)
model.rf
# visualize the out-of-bag error
# create a dataframe for visulizing the error rate
oob.df = data.frame(
Tree=rep(1:nrow(model.rf$err.rate), times=3),
Type = rep(c('OOB','oak','pine'), each=nrow(model.rf$err.rate)),
Error=c(model.rf$err.rate[,'OOB'],
model.rf$err.rate[,'oak'],model.rf$err.rate[,'pine'])
)
model.rf$err.rate
# visualize the out-of-bag error
# create a dataframe for visulizing the error rate
oob.df = data.frame(
Tree=rep(1:nrow(model.rf$err.rate), times=3),
Type = rep(c('OOB','1','2','3'), each=nrow(model.rf$err.rate)),
Error=c(model.rf$err.rate[,'OOB'], model.rf$err.rate[,'1'],
model.rf$err.rate[,'2'],model.rf$err.rate[,'3'])
)
# visualize the out-of-bag error
# create a dataframe for visulizing the error rate
oob.df = data.frame(
Tree=rep(1:nrow(model.rf$err.rate), times=3),
Type = rep(c('OOB','1','2','3'), each=nrow(model.rf$err.rate)),
Error=c(model.rf$err.rate[,'OOB'], model.rf$err.rate[,'1'],
model.rf$err.rate[,'2'],model.rf$err.rate[,'3'])
)
model.rf$err.rate
nrow(model.rf$err.rate)
is.na(model.rf$err.rate)
# visualize the out-of-bag error
# create a dataframe for visulizing the error rate
oob.df = data.frame(
Tree=rep(1:nrow(model.rf$err.rate), times=3),
Type = rep(c('OOB','1','2','3'), each=nrow(model.rf$err.rate)),
Error=c(model.rf$err.rate[,'OOB'], model.rf$err.rate[,'1'],
model.rf$err.rate[,'2'],model.rf$err.rate[,'3'])
)
oob.df = data.frame(Tree=rep(1:nrow(model.rf$err.rate), times=3))
View(oob.df)
rep(c('OOB','1','2','3'), each=nrow(model.rf$err.rate))
View(oob.df)
# visualize the out-of-bag error
# create a dataframe for visulizing the error rate
oob.df = data.frame(
Tree=rep(1:nrow(model.rf$err.rate), times=4),
Type = rep(c('OOB','1','2','3'), each=nrow(model.rf$err.rate)),
Error=c(model.rf$err.rate[,'OOB'], model.rf$err.rate[,'1'],
model.rf$err.rate[,'2'],model.rf$err.rate[,'3'])
)
View(oob.df)
ggplot(data=oob.df,aes(x=Tree, y=Error))+
geom_line(aes(color=Type))
library(ggplot2)
ggplot(data=oob.df,aes(x=Tree, y=Error))+
geom_line(aes(color=Type))
# create a dataframe for visulizing the error rate
oob.df = data.frame(
Tree=rep(1:nrow(model.rf$err.rate), times=4),
Type = rep(c('OOB','Group 1',' Group 2','Group 3'), each=nrow(model.rf$err.rate)),
Error=c(model.rf$err.rate[,'OOB'], model.rf$err.rate[,'1'],
model.rf$err.rate[,'2'],model.rf$err.rate[,'3'])
)
ggplot(data=oob.df,aes(x=Tree, y=Error))+
geom_line(aes(color=Type))
# visualize the out-of-bag error
# create a dataframe for visulizing the error rate
oob.df = data.frame(
Tree=rep(1:nrow(model.rf$err.rate), times=4),
Type = rep(c('OOB','Group 1','Group 2','Group 3'), each=nrow(model.rf$err.rate)),
Error=c(model.rf$err.rate[,'OOB'], model.rf$err.rate[,'1'],
model.rf$err.rate[,'2'],model.rf$err.rate[,'3'])
)
ggplot(data=oob.df,aes(x=Tree, y=Error))+
geom_line(aes(color=Type))
ggplot(data=oob.df,aes(x=Tree, y=Error))+
geom_line(aes(color=Type))+theme_bw()
ggplot(data=oob.df,aes(x=Tree, y=Error))+
geom_line(aes(color=Type))+theme_bw()+ labs(x='Number of trees')
ggplot(data=oob.df,aes(x=Tree, y=Error))+
geom_line(aes(color=Type))+theme_bw()+ labs(x='Number of trees')+
ylim(0,0.75)
png(filename = paste0(folder_result,"13_randomForest/01_oob.png"),
width = 600, height = 600, res = 120)
ggplot(data=oob.df,aes(x=Tree, y=Error))+
geom_line(aes(color=Type))+theme_bw()+ labs(x='Number of trees')+
ylim(0,0.75)
dev.off()
png(filename = paste0(folder_result,"13_randomForest/01_oob.png"),
width = 600, height = 500, res = 120)
ggplot(data=oob.df,aes(x=Tree, y=Error))+
geom_line(aes(color=Type))+theme_bw()+ labs(x='Number of trees')+
ylim(0,0.75)
dev.off()
# Now check to see if the random forest better with different parameters
# by comparing OOB value
# try different ntree of 200,400,600,800, and 1000
# try different mtry of 1, 2, 3, and 4
oob=c()
ntree = c(200,400,600,800,1000)
for (tree in ntree) {
for (nvariable in 1:2) {
model.rf = randomForest(cluster ~ PC1 + PC2,
data = train,
keep.forest = T,
importance = TRUE,
mtry = nvariable,
ntree = tree)
print(paste0("ntree = ",tree))
print(paste0('mtry = ',nvariable))
oob[length(oob)+1] = mean(model.rf$err.rate)
oob[length(oob)]
}
}
# adding colnames and rownnames for better visualization
oob = matrix(oob, ncol=2, byrow = T)
colnames(oob) = c('mtry = 1','mtry = 2')
rownames(oob) = c('ntree = 200','ntree = 400','ntree = 600','ntree = 800','ntree = 1000')
oob
# Now check to see if the random forest better with different parameters
# by comparing OOB value
# try different ntree of 200,400,600,800, and 1000
# try different mtry of 1, 2, 3, and 4
oob=c()
ntree = c(100,200,300,400,500)
for (tree in ntree) {
for (nvariable in 1:2) {
model.rf = randomForest(cluster ~ PC1 + PC2,
data = train,
keep.forest = T,
importance = TRUE,
mtry = nvariable,
ntree = tree)
print(paste0("ntree = ",tree))
print(paste0('mtry = ',nvariable))
oob[length(oob)+1] = mean(model.rf$err.rate)
oob[length(oob)]
}
}
# adding colnames and rownnames for better visualization
oob = matrix(oob, ncol=2, byrow = T)
colnames(oob) = c('mtry = 1','mtry = 2')
rownames(oob) = c('ntree = 200','ntree = 400','ntree = 600','ntree = 800','ntree = 1000')
oob
# adding colnames and rownnames for better visualization
oob = matrix(oob, ncol=2, byrow = T)
colnames(oob) = c('mtry = 1','mtry = 2')
rownames(oob) = c('ntree = 100','ntree = 200','ntree = 300','ntree = 400','ntree = 5000')
oob
mean(model.rf$err.rate)
model.rf = randomForest(cluster ~ PC1 + PC2,
data = train,
keep.forest = T,
importance = T,
proximity = T)
(model.rf$err.rate)
mean(model.rf$err.rate)
model.rf$err.rate
model.rf$err.rate[500,1]
for (tree in ntree) {
for (nvariable in 1:2) {
model.rf = randomForest(cluster ~ PC1 + PC2,
data = train,
keep.forest = T,
importance = TRUE,
mtry = nvariable,
ntree = tree)
print(paste0("ntree = ",tree))
print(paste0('mtry = ',nvariable))
oob[length(oob)+1] = mean(model.rf$err.rate)
oob[length(oob)]
}
}
oob[length(oob)]
oob
# Now check to see if the random forest better with different parameters
# by comparing OOB value
# try different ntree of 200,400,600,800, and 1000
# try different mtry of 1, 2, 3, and 4
oob=c()
ntree = c(100,200,300,400,500)
for (tree in ntree) {
for (nvariable in 1:2) {
model.rf = randomForest(cluster ~ PC1 + PC2,
data = train,
keep.forest = T,
importance = TRUE,
mtry = nvariable,
ntree = tree)
print(paste0("ntree = ",tree))
print(paste0('mtry = ',nvariable))
oob[length(oob)+1] = model.rf$err.rate[nrow(model.rf$err.rate),1]
oob[length(oob)]
}
}
# adding colnames and rownnames for better visualization
oob = matrix(oob, ncol=2, byrow = T)
oob
colnames(oob) = c('mtry = 1','mtry = 2')
rownames(oob) = c('ntree = 100','ntree = 200','ntree = 300','ntree = 400','ntree = 5000')
oob
model.rf = randomForest(cluster ~ PC1 + PC2,
data = train,
keep.forest = T,
importance = T,
proximity = T)
# Now check to see if the random forest better with different parameters
# by comparing OOB value
# try different ntree of 200,400,600,800, and 1000
# try different mtry of 1, 2, 3, and 4
oob=c()
ntree = c(100,200,300,400,500)
for (tree in ntree) {
for (nvariable in 1:2) {
model.rf = randomForest(cluster ~ PC1 + PC2,
data = train,
keep.forest = T,
importance = TRUE,
mtry = nvariable,
ntree = tree)
print(paste0("ntree = ",tree))
print(paste0('mtry = ',nvariable))
oob[length(oob)+1] = model.rf$err.rate[nrow(model.rf$err.rate),1]
oob[length(oob)]
}
}
# adding colnames and rownnames for better visualization
oob = matrix(oob, ncol=2, byrow = T)
colnames(oob) = c('mtry = 1','mtry = 2')
rownames(oob) = c('ntree = 100','ntree = 200','ntree = 300','ntree = 400','ntree = 5000')
oob
# final random forest with chosen parameter of mtry = 3 and ntree = 600
model.rf = randomForest(Species ~ DBH + H + V + Age,
data = train,
keep.forest = T,
importance = TRUE,
mtry = 1,
ntree = 300)
# final random forest with chosen parameter of mtry = 3 and ntree = 600
model.rf = randomForest(cluster ~ PC1 + PC2,
data = train,
keep.forest = T,
importance = TRUE,
mtry = 1,
ntree = 300)
model.rf
install.packages("caret")
library(caret)
# tree nodes distributions
hist(treesize(model.rf))
prp(model.rf)
plot(model.rf)
# plot importance of each predictor variables
varImpPlot(model.rf)
png(filename = paste0(folder_result,"13_randomForest/02_vaiableImportant.png"),
width = 600, height = 500, res = 120)
varImpPlot(model.rf)
dev.off()
png(filename = paste0(folder_result,"13_randomForest/02_vaiableImportant.png"),
width = 700, height = 500, res = 120)
varImpPlot(model.rf)
dev.off()
MDSplot(model.rf, train$cluster)
model.rf = randomForest(cluster ~ PC1 + PC2,
data = train,
keep.forest = T,
importance = TRUE,
proximity =T,
mtry = nvariable,
ntree = tree)
MDSplot(model.rf, train$cluster)
# classify the test dataset
prediction<-predict(model.rf, test ,type = 'class')
# accuracy assessment
confusionMatrix(prediction, test$cluster)
prediction
test
prediction
# accuracy assessment
confusionMatrix(prediction, test$cluster)
# accuracy assessment
confusionMatrix(prediction, test$cluster)
prediction
summary(prediction)
test
pc.df2 = pc.df[pc.df$tile_id==c('634_5608','637_5609')]
pc.df2 = pc.df[pc.df$tile_id==c('634_5608','637_5609'),]
View(pc.df2)
pc.df2 = pc.df[pc.df$tile_id=c('634_5608','637_5609'),]
View(pc.df)
summary(pc.df)
pc.df$tile_id==c('634_5608','637_5609')
pc.df2 = pc.df[pc.df$tile_id=="634_5608",]
pc.df2 = pc.df[pc.df$tile_id=="634_5608"/"637_5609",]
pc.df2 = pc.df[pc.df$tile_id=="634_5608" or "637_5609",]
pc.df2 = pc.df[pc.df$tile_id=="634_5608"/"637_5609",]
pc.df$tile_id
pc.df2 = pc.df[pc.df$tile_id["634_5608","637_5609"],]
pc.df$tile_id["634_5608","637_5609"]
pc.df$tile_id
pc.df$tile_id[c("634_5608","637_5609")]
pc.df2 = pc.df[pc.df$tile_id[c("634_5608","637_5609")],]
View(pc.df2)
pc.df$tile_id[c("634_5608","637_5609")]
pc.df$tile_id["634_5608","637_5609"]
pc.df$tile_id[["634_5608","637_5609"]]
pc.df$tile_id[["634_5608","637_5609"]]
pc.df$tile_id["634_5608"]
pc.df$tile_id
pc.df$tile_id["634_5608",]
pc.df$tile_id[1]
pc.df$tile_id["614_5608"]
pc.df2 = pc.df[pc.df$tile_id=='634_5608' and pc.df$tile_id== '637_5609',]
pc.df2 = pc.df[pc.df$tile_id=='634_5608' & pc.df$tile_id== '637_5609',]
View(pc.df2)
pc.df2 = pc.df[pc.df$tile_id=='634_5608',]
View(pc.df2)
pc.df2 = pc.df[pc.df$tile_id=='634_5608'&'637_5609',]
pc.df2 = pc.df[pc.df$tile_id==c('634_5608','637_5609'),]
pc.df2 = pc.df[pc.df$tile_id=c('634_5608','637_5609'),]
pc.df2 = subset(pc.df, tile_id==c('634_5608','637_5609'))
View(pc.df2)
pc.df2 = subset(pc.df, tile_id=='634_5608' & '637_5609')
pc.df2 = subset(pc.df, pc.df$tile_id =='634_5608' & '637_5609')
pc.df$tile_id==c('634_5608','637_5609')
pc.df2 = pc.df[which(pc.df$tile_id==c('634_5608','637_5609')),]
pc.df2 = pc.df[pc.df$tile_id=='634_5608',]
View(pc.df2)
View(pc.df2)
test
pc.df2 = pc.df[pc.df$tile_id=='634_5608'|pc.df$tile_id=='637_5609',]
View(pc.df2)
pc.df2 = pc.df[pc.df$tile_id=='634_5608'|'637_5609',]
pc.df2 = pc.df[pc.df$tile_id==c('634_5608'|'637_5609'),]
pc.df2 = pc.df[pc.df$tile_id=='634_5608'|'637_5609',]
pc.df2 = pc.df[pc.df$tile_id=='634_5608'|pc.df$tile_id=='637_5609',]
View(pc.df2)
pc.df
pc.df2
pca = prcomp(pc.df2[1:2], center = TRUE, scale. = TRUE)
# PCA
# by default, prcomp() expects the samples to be rows
# and the attributes to be columns
pca = prcomp(metric.df[2:28], center = TRUE, scale. = TRUE)
pca2 = prcomp(pc.df2[1:2], center = TRUE, scale. = TRUE)
plot(pca2$x[,1], pca2$x[,2])
pca2$x[,1]
pca2$x
View(pc.df)
plot(pca2$x[,1], pca2$x[,2])
prediction = predict(model.rf, pc.df2, type='class')
# classify the test dataset
prediction<-predict(model.rf, test ,type = 'class')
prediction2 = predict(model.rf, pc.df2, type='class')
prediction2
View(pc.df2)
View(test)
View(metric.df)
pc.df2 = metric.df[metric.df$tile_id=='634_5608'|metric.df$tile_id=='637_5609',]
View(pc.df2)
pca2 = prcomp(pc.df2[2:28], center = TRUE, scale. = TRUE)
plot(pca2$x[,1], pca2$x[,2])
pc.df2
pca2$x
pc = pca2$x[,1:2]
pc.df = as.data.frame(pc)
View(test)
metric.df2 = metric.df[metric.df$tile_id=='634_5608'|metric.df$tile_id=='637_5609',]
pca2 = prcomp(metric.df2[2:28], center = TRUE, scale. = TRUE)
plot(pca2$x[,1], pca2$x[,2])
pc2 = pca2$x[,1:2]
pc.df2 = as.data.frame(pc)
prediction2 = predict(model.rf, pc.df2, type='class')
prediction2
pc = pca$x[,1:2]
pc.df = as.data.frame(pc)
pc.df$cluster = kmeans.3$cluster
pc.df$tile_id = metric.df$tile_id
# Set random seed to make results reproducible:
set.seed(200)
# split data into two 80% for training and 20% for testing
sample <- sample(2, nrow(pc.df), replace=T, prob=c(0.8,0.2))
train<- pc.df[sample==1,]
test<- pc.df[sample==2,]
nrow(train)
nrow(test)
# convert class to factor
train$cluster = as.factor(train$cluster)
# load library
model.rf = randomForest(cluster ~ PC1 + PC2,
data = train,
keep.forest = T,
importance = T,
proximity = T)
model.rf
# visualize the out-of-bag error
# create a dataframe for visulizing the error rate
oob.df = data.frame(
Tree=rep(1:nrow(model.rf$err.rate), times=4),
Type = rep(c('OOB','Group 1',' Group 2','Group 3'), each=nrow(model.rf$err.rate)),
Error=c(model.rf$err.rate[,'OOB'], model.rf$err.rate[,'1'],
model.rf$err.rate[,'2'],model.rf$err.rate[,'3'])
)
png(filename = paste0(folder_result,"13_randomForest/01_oob.png"),
width = 800, height = 500, res = 120)
ggplot(data=oob.df,aes(x=Tree, y=Error))+
geom_line(aes(color=Type))+theme_bw()+ labs(x='Number of trees')+
ylim(0,0.75)
dev.off()
